{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794ee32c",
   "metadata": {},
   "source": [
    "## Install Depedencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89691097",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torchvision datasets pillow evaluate rouge_score pycocoevalcap bert_score --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e1d1978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paracha\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    CLIPVisionModel, CLIPProcessor,\n",
    "    OPTForCausalLM, AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "############### Flickr 8K Class for All Captions ####################\n",
    "\n",
    "\n",
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(self, images_dir, captions_file, tokenizer,\n",
    "                 prefix=\"a photo of\", max_length=40, image_size=224):\n",
    "        self.images_dir = images_dir\n",
    "        self.tokenizer  = tokenizer\n",
    "        self.prefix     = prefix\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.samples = []  \n",
    "        with open(captions_file, 'r') as f:\n",
    "            for line in f:\n",
    "                key, cap = line.strip().split('\\t', 1)\n",
    "                filename = key.split('#')[0]\n",
    "                self.samples.append((filename, cap))\n",
    "\n",
    "        # Precompute the image transform\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                        std=(0.26862954, 0.26130258, 0.27577711)),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename, caption = self.samples[idx]\n",
    "        img_path     = os.path.join(self.images_dir, filename)\n",
    "        image        = Image.open(img_path).convert('RGB')\n",
    "        pixel_values = self.transform(image)  # (3, H, W)\n",
    "\n",
    "        full = f\"{self.prefix} {caption}\"\n",
    "        toks = self.tokenizer(\n",
    "            full,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids      = toks.input_ids.squeeze(0)      # (T,)\n",
    "        attention_mask = toks.attention_mask.squeeze(0) # (T,)\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        prefix_ids = self.tokenizer(\n",
    "            self.prefix,\n",
    "            add_special_tokens=False\n",
    "        )['input_ids']\n",
    "        labels[:len(prefix_ids)] = -100\n",
    "\n",
    "        return pixel_values, input_ids, attention_mask, labels\n",
    "\n",
    "\n",
    "\n",
    "class QFormer(nn.Module):\n",
    "    def __init__(self, image_feat_dim=768, query_dim=768,\n",
    "                 num_queries=32, num_heads=8, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.num_queries = num_queries\n",
    "        self.query_dim = query_dim\n",
    "        self.queries = nn.Parameter(torch.randn(num_queries, query_dim))\n",
    "        self.cross_attn = nn.ModuleList([\n",
    "            nn.MultiheadAttention(query_dim, num_heads)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.self_attn = nn.ModuleList([\n",
    "            nn.MultiheadAttention(query_dim, num_heads)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.proj = nn.Linear(image_feat_dim, query_dim)\n",
    "        self.ffns = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(query_dim, query_dim * 4),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(query_dim * 4, query_dim)\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm1 = nn.ModuleList([nn.LayerNorm(query_dim) for _ in range(num_layers)])\n",
    "        self.norm2 = nn.ModuleList([nn.LayerNorm(query_dim) for _ in range(num_layers)])\n",
    "        self.norm3 = nn.ModuleList([nn.LayerNorm(query_dim) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, image_feats):\n",
    "        \"\"\"\n",
    "        image_feats: (B, S, D_img)\n",
    "        returns: (B, num_queries, query_dim)\n",
    "        \"\"\"\n",
    "        B, S, D = image_feats.size()\n",
    "        proj_feats = self.proj(image_feats)          # (B, S, query_dim)\n",
    "        proj_feats = proj_feats.permute(1, 0, 2)      # (S, B, Q)\n",
    "        q = self.queries.unsqueeze(1).repeat(1, B, 1) # (num_q, B, Q)\n",
    "        for i in range(len(self.cross_attn)):\n",
    "            q2, _ = self.cross_attn[i](\n",
    "                query=q,\n",
    "                key=proj_feats,\n",
    "                value=proj_feats\n",
    "            )\n",
    "            q = self.norm1[i](q + q2)\n",
    "            q2, _ = self.self_attn[i](\n",
    "                query=q, key=q, value=q\n",
    "            )\n",
    "            q = self.norm2[i](q + q2)\n",
    "            q2 = self.ffns[i](q)\n",
    "            q = self.norm3[i](q + q2)\n",
    "        q = q.permute(1, 0, 2)  # (B, num_q, Q)\n",
    "        return q\n",
    "\n",
    "\n",
    "class BLIP2Captioning(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vision_model_name=\"openai/clip-vit-base-patch32\",\n",
    "                 llm_model_name=\"facebook/opt-125m\",\n",
    "                 num_queries=32,\n",
    "                 query_dim=768):\n",
    "        super().__init__()\n",
    "        self.num_queries = num_queries\n",
    "\n",
    "        self.vision = CLIPVisionModel.from_pretrained(vision_model_name)\n",
    "        for p in self.vision.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        img_feat_dim = self.vision.config.hidden_size\n",
    "        self.qformer = QFormer(\n",
    "            image_feat_dim=img_feat_dim,\n",
    "            query_dim=query_dim,\n",
    "            num_queries=num_queries,\n",
    "            num_heads=8,\n",
    "            num_layers=4\n",
    "        )\n",
    "\n",
    "        self.llm = OPTForCausalLM.from_pretrained(llm_model_name)\n",
    "        for p in self.llm.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        lm_dim = self.llm.config.hidden_size\n",
    "        self.vis_proj = nn.Linear(query_dim, lm_dim)\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask, labels=None):\n",
    "        v = self.vision(pixel_values=pixel_values).last_hidden_state  # (B, S, D_img)\n",
    "\n",
    "        q = self.qformer(v)                                           # (B, Q, query_dim)\n",
    "\n",
    "        vis_emb = self.vis_proj(q)                                    # (B, Q, lm_dim)\n",
    "\n",
    "        tok_emb = self.llm.model.decoder.embed_tokens(input_ids)      # (B, T, lm_dim)\n",
    "\n",
    "        inputs_embeds = torch.cat([vis_emb, tok_emb], dim=1)          # (B, Q+T, lm_dim)\n",
    "\n",
    "        vp_mask = inputs_embeds.new_ones((inputs_embeds.size(0), vis_emb.size(1)))\n",
    "        attn_mask = torch.cat([vp_mask, attention_mask], dim=1)       # (B, Q+T)\n",
    "\n",
    "        if labels is not None:\n",
    "            B, T = labels.size()\n",
    "            Q = self.num_queries\n",
    "            pad = torch.full((B, Q), -100, device=labels.device, dtype=labels.dtype)\n",
    "            labels = torch.cat([pad, labels], dim=1)                  # (B, Q+T)\n",
    "\n",
    "        out = self.llm(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attn_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "108d6952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40455\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "device       = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "images_dir = \"Dataset/Images\"\n",
    "captions_file= \"Dataset/Flickr8k.token.txt\"\n",
    "\n",
    "# hyperparams\n",
    "batch_size   = 64\n",
    "lr           = 1e-4\n",
    "epochs       = 100\n",
    "max_len      = 40\n",
    "warmup_steps = 1000\n",
    "\n",
    "\n",
    "model = BLIP2Captioning().to(device)\n",
    "tokenizer = model.tokenizer\n",
    "ds = Flickr8kDataset(images_dir, captions_file,\n",
    "                      tokenizer,\n",
    "                      max_length=max_len,\n",
    "                      image_size=224)\n",
    "ds_small = Subset(ds, list(range(10000)))\n",
    "print(len(ds))\n",
    "\n",
    "def train():\n",
    "    # paths\n",
    "\n",
    "    loader = DataLoader(ds_small, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.qformer.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=epochs * len(loader)\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for pix, ids, am, labs in loader:\n",
    "            pix, ids, am, labs = pix.to(device), ids.to(device), am.to(device), labs.to(device)\n",
    "            out = model(pix, ids, am, labels=labs)\n",
    "            loss = out.loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.qformer.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs} — avg loss: {total_loss/len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2218a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train() ## train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd68822",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"blip2_flickr8k_10kexamples.pth\") ## save the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5604f826",
   "metadata": {},
   "source": [
    "## Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28d1cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/Images/1000268201_693b08cb0e.jpg → A little girl climbing into a wooden playhouse .\n",
      "Dataset/Images/10815824_2997e03d76.jpg → A girl and her horse stand by a fire .\n",
      "Dataset/Images/3726168984_1fa2c8965b.jpg → Two black dogs running in the grass .\n",
      "Dataset/Images/3744832122_2f4febdff6.jpg → A boy plays a baseball game .\n",
      "Dataset/Images/3726168984_1fa2c8965b.jpg → Two black dogs running in the grass .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def load_model(checkpoint_path, device):\n",
    "    model = BLIP2Captioning().to(device)\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def preprocess_image(image_path, image_size, device):\n",
    "    transform = T.Compose([\n",
    "        T.Resize((image_size, image_size)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                    std=(0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    pix = transform(img).unsqueeze(0).to(device)  # (1, 3, H, W)\n",
    "    return pix\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_caption(model, image_path,\n",
    "                     device,\n",
    "                     prefix=\"a photo of\",\n",
    "                     max_len=40,\n",
    "                     num_beams=3):\n",
    "    pix = preprocess_image(image_path, image_size=224, device=device)\n",
    "\n",
    "    v = model.vision(pixel_values=pix).last_hidden_state      # (1, S, D_img)\n",
    "\n",
    "    q = model.qformer(v)                                      # (1, Q, query_dim)\n",
    "\n",
    "    vis_emb = model.vis_proj(q)                               # (1, Q, lm_dim)\n",
    "\n",
    "    # tok = model.tokenizer(prefix, return_tensors='pt').to(device)\n",
    "    tok = model.tokenizer(\n",
    "    prefix,\n",
    "    return_tensors='pt',\n",
    "    add_special_tokens=False     \n",
    "      ).to(device)\n",
    "    prefix_emb = model.llm.model.decoder.embed_tokens(tok.input_ids)\n",
    "    inputs_embeds = torch.cat([vis_emb, prefix_emb], dim=1)   # (1, Q+P, lm_dim)\n",
    "\n",
    "    vp_mask = torch.ones((1, vis_emb.size(1)), device=device)\n",
    "    attn_mask = torch.cat([vp_mask, tok.attention_mask], dim=1)\n",
    "\n",
    "    out = model.llm.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=attn_mask,\n",
    "        # max_length=vis_emb.size(1) + tok.input_ids.size(1) + max_len,\n",
    "        max_new_tokens=max_len,             # generate up to `max_len` new tokens\n",
    "        num_beams=num_beams,\n",
    "        eos_token_id=model.tokenizer.eos_token_id,\n",
    "        pad_token_id=model.tokenizer.pad_token_id,\n",
    "        early_stopping=True,\n",
    "        do_sample = True,\n",
    "        top_p = 0.9,\n",
    "        # top_k = 50,\n",
    "        temperature = 0.7,\n",
    "    )  \n",
    "\n",
    "\n",
    "    q, p = vis_emb.size(1), tok.input_ids.size(1)\n",
    "    gen_ids = out\n",
    "\n",
    "    caption = model.tokenizer.decode(gen_ids[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    return caption\n",
    "\n",
    "# Example usage:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = load_model(\"blip2_flickr8k_10kexamples.pth\", device)\n",
    "\n",
    "for img_path in [\n",
    "    \"Dataset/Images/1000268201_693b08cb0e.jpg\",\n",
    "    \"Dataset/Images/10815824_2997e03d76.jpg\",\n",
    "    \"Dataset/Images/3726168984_1fa2c8965b.jpg\",\n",
    "    \"Dataset/Images/3744832122_2f4febdff6.jpg\",\n",
    "    \"Dataset/Images/3726168984_1fa2c8965b.jpg\",\n",
    "   \n",
    "\n",
    "\n",
    "]:\n",
    "    cap = generate_caption(model, img_path, device)\n",
    "    print(img_path, \"→\", cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7fb658",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c1b2400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss over 1000 images: 1.2649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 5.94k/5.94k [00:00<00:00, 778kB/s]\n",
      "Downloading extra modules: 4.07kB [00:00, 1.61MB/s]                   \n",
      "Downloading extra modules: 100%|██████████| 3.34k/3.34k [00:00<?, ?B/s]\n",
      "Downloading builder script: 100%|██████████| 7.02k/7.02k [00:00<?, ?B/s]\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Paracha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Paracha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Paracha\\AppData\\Roaming\\nltk_data...\n",
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 6.29MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading stanford-corenlp-3.6.0 for SPICE ...\n",
      "Progress: 384.5M / 384.5M (100.0%)\n",
      "Extracting stanford-corenlp-3.6.0 ...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 7.95k/7.95k [00:00<?, ?B/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "c:\\Users\\Paracha\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Paracha\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch32. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF-BLEU-4: 0.1809\n",
      "METEOR : 0.3031\n",
      "ROUGE-L: 0.3446\n",
      "BLEU-1: 0.6378  BLEU-2: 0.4274\n",
      "BLEU-3: 0.2840  BLEU-4: 0.1809\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['java', '-jar', '-Xmx8G', 'spice-1.0.jar', 'c:\\\\Users\\\\Paracha\\\\miniconda3\\\\Lib\\\\site-packages\\\\pycocoevalcap\\\\spice\\\\tmp\\\\tmp22nzhjrd', '-cache', 'c:\\\\Users\\\\Paracha\\\\miniconda3\\\\Lib\\\\site-packages\\\\pycocoevalcap\\\\spice\\\\cache', '-out', 'c:\\\\Users\\\\Paracha\\\\miniconda3\\\\Lib\\\\site-packages\\\\pycocoevalcap\\\\spice\\\\tmp\\\\tmp9aier1tg', '-subset', '-silent']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 104\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEU-3: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbleu3\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  BLEU-4: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbleu4\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    100\u001b[0m cider_score, _ \u001b[38;5;241m=\u001b[39m cider\u001b[38;5;241m.\u001b[39mcompute_score(\n\u001b[0;32m    101\u001b[0m     {i: refs[i]    \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(refs))},\n\u001b[0;32m    102\u001b[0m     {i: [preds[i]] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(preds))}\n\u001b[0;32m    103\u001b[0m )\n\u001b[1;32m--> 104\u001b[0m spice_score, _ \u001b[38;5;241m=\u001b[39m \u001b[43mspice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCIDEr : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcider_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPICE : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspice_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Paracha\\miniconda3\\Lib\\site-packages\\pycocoevalcap\\spice\\spice.py:75\u001b[0m, in \u001b[0;36mSpice.compute_score\u001b[1;34m(self, gts, res)\u001b[0m\n\u001b[0;32m     68\u001b[0m   os\u001b[38;5;241m.\u001b[39mmakedirs(cache_dir)\n\u001b[0;32m     69\u001b[0m spice_cmd \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjava\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-jar\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-Xmx8G\u001b[39m\u001b[38;5;124m'\u001b[39m, SPICE_JAR, in_file\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m     70\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-cache\u001b[39m\u001b[38;5;124m'\u001b[39m, cache_dir,\n\u001b[0;32m     71\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-out\u001b[39m\u001b[38;5;124m'\u001b[39m, out_file\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m     72\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-subset\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     73\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-silent\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     74\u001b[0m ]\n\u001b[1;32m---> 75\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspice_cmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabspath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Read and process results\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(out_file\u001b[38;5;241m.\u001b[39mname) \u001b[38;5;28;01mas\u001b[39;00m data_file:    \n",
      "File \u001b[1;32mc:\\Users\\Paracha\\miniconda3\\Lib\\subprocess.py:413\u001b[0m, in \u001b[0;36mcheck_call\u001b[1;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cmd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    412\u001b[0m         cmd \u001b[38;5;241m=\u001b[39m popenargs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 413\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, cmd)\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['java', '-jar', '-Xmx8G', 'spice-1.0.jar', 'c:\\\\Users\\\\Paracha\\\\miniconda3\\\\Lib\\\\site-packages\\\\pycocoevalcap\\\\spice\\\\tmp\\\\tmp22nzhjrd', '-cache', 'c:\\\\Users\\\\Paracha\\\\miniconda3\\\\Lib\\\\site-packages\\\\pycocoevalcap\\\\spice\\\\cache', '-out', 'c:\\\\Users\\\\Paracha\\\\miniconda3\\\\Lib\\\\site-packages\\\\pycocoevalcap\\\\spice\\\\tmp\\\\tmp9aier1tg', '-subset', '-silent']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "import os, random, torch\n",
    "from collections import defaultdict\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "import evaluate\n",
    "from pycocoevalcap.spice.spice import Spice\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "smooth = SmoothingFunction().method4\n",
    "\n",
    "model = load_model(\"blip2_flickr8k_10kexamples.pth\", device)\n",
    "model.eval()\n",
    "\n",
    "full_ds = Flickr8kDataset(\n",
    "    images_dir=images_dir,\n",
    "    captions_file=captions_file,\n",
    "    tokenizer=model.tokenizer,\n",
    "    prefix=\"a photo of\",\n",
    "    max_length=40,\n",
    "    image_size=224\n",
    ")\n",
    "\n",
    "img2refs = defaultdict(list)\n",
    "for fname, cap in full_ds.samples:  # full_ds.samples is list of (filename, caption)\n",
    "    img2refs[fname].append(cap)\n",
    "unique_fnames = list(img2refs.keys())\n",
    "\n",
    "n_caption_val = 5000\n",
    "# each image has 5 captions → how many *unique* images?\n",
    "n_unique_val = n_caption_val // 5\n",
    "val_fnames = unique_fnames[-n_unique_val:]\n",
    "\n",
    "val_pairs = [(fn, random.choice(img2refs[fn])) for fn in val_fnames]\n",
    "\n",
    "val_loss = 0.0\n",
    "for fn, gt in val_pairs:\n",
    "    img_path = os.path.join(images_dir, fn)\n",
    "    pix = preprocess_image(img_path, image_size=224, device=device)\n",
    "    full = full_ds.prefix + \" \" + gt\n",
    "    toks = model.tokenizer(full,\n",
    "                           padding='max_length',\n",
    "                           truncation=True,\n",
    "                           max_length=full_ds.max_length,\n",
    "                           return_tensors='pt').to(device)\n",
    "    input_ids = toks.input_ids\n",
    "    am        = toks.attention_mask\n",
    "    labels = input_ids.clone()\n",
    "    prefix_len = len(model.tokenizer(full_ds.prefix,\n",
    "                                     add_special_tokens=False)['input_ids'])\n",
    "    labels[:, :prefix_len] = -100\n",
    "    out = model(pix, input_ids, am, labels=labels)\n",
    "    val_loss += out.loss.item()\n",
    "\n",
    "avg_val_loss = val_loss / len(val_pairs)\n",
    "print(f\"Validation loss over {len(val_pairs)} images: {avg_val_loss:.4f}\")\n",
    "\n",
    "bleu_hf   = evaluate.load(\"bleu\")\n",
    "meteor    = evaluate.load(\"meteor\")\n",
    "rouge     = evaluate.load(\"rouge\")\n",
    "spice     = Spice()\n",
    "cider     = Cider()\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "clip_model= CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_proc = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "preds = []\n",
    "refs = []  \n",
    "for fn in val_fnames:\n",
    "    img_path = os.path.join(images_dir, fn)\n",
    "    pred = generate_caption(\n",
    "        model, img_path, device,\n",
    "        prefix=full_ds.prefix,\n",
    "        max_len=40,\n",
    "        num_beams=3\n",
    "    )\n",
    "    preds.append(pred)\n",
    "    refs.append(img2refs[fn])\n",
    "\n",
    "bleu_res   = bleu_hf.compute(predictions=preds, references=refs)\n",
    "meteor_res = meteor.compute(predictions=preds,\n",
    "                            references=[r[0] for r in refs])\n",
    "rouge_res  = rouge.compute(predictions=preds,\n",
    "                           references=[r[0] for r in refs])\n",
    "print(f\"HF-BLEU-4: {bleu_res['bleu']:.4f}\")\n",
    "print(f\"METEOR : {meteor_res['meteor']:.4f}\")\n",
    "print(f\"ROUGE-L: {rouge_res['rougeL']:.4f}\")\n",
    "\n",
    "pred_toks = [p.split() for p in preds]\n",
    "ref_toks  = [[r.split() for r in rs] for rs in refs]\n",
    "bleu1 = corpus_bleu(ref_toks, pred_toks, weights=(1,0,0,0), smoothing_function=smooth)\n",
    "bleu2 = corpus_bleu(ref_toks, pred_toks, weights=(0.5,0.5,0,0), smoothing_function=smooth)\n",
    "bleu3 = corpus_bleu(ref_toks, pred_toks, weights=(0.33,0.33,0.33,0), smoothing_function=smooth)\n",
    "bleu4 = corpus_bleu(ref_toks, pred_toks, weights=(0.25,0.25,0.25,0.25), smoothing_function=smooth)\n",
    "print(f\"BLEU-1: {bleu1:.4f}  BLEU-2: {bleu2:.4f}\")\n",
    "print(f\"BLEU-3: {bleu3:.4f}  BLEU-4: {bleu4:.4f}\")\n",
    "\n",
    "cider_score, _ = cider.compute_score(\n",
    "    {i: refs[i]    for i in range(len(refs))},\n",
    "    {i: [preds[i]] for i in range(len(preds))}\n",
    ")\n",
    "spice_score, _ = spice.compute_score(\n",
    "    {i: refs[i]    for i in range(len(refs))},\n",
    "    {i: [preds[i]] for i in range(len(preds))}\n",
    ")\n",
    "print(f\"CIDEr : {cider_score:.4f}\")\n",
    "print(f\"SPICE : {spice_score:.4f}\")\n",
    "\n",
    "bert_res = bertscore.compute(\n",
    "    predictions=preds,\n",
    "    references=[r[0] for r in refs],\n",
    "    model_type=\"bert-base-uncased\",\n",
    "    device=device\n",
    ")\n",
    "f1 = sum(bert_res[\"f1\"]) / len(bert_res[\"f1\"])\n",
    "print(f\"BERTScore-F1: {f1:.4f}\")\n",
    "\n",
    "all_sims = []\n",
    "batch_size = 32\n",
    "for i in range(0, len(val_fnames), batch_size):\n",
    "    chunk = val_fnames[i : i + batch_size]\n",
    "    imgs  = [Image.open(os.path.join(images_dir, fn)).convert(\"RGB\") for fn in chunk]\n",
    "    txts  = [preds[val_fnames.index(fn)] for fn in chunk]\n",
    "    inputs = clip_proc(text=txts, images=imgs, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        iv = clip_model.get_image_features(pixel_values=inputs.pixel_values)\n",
    "        tv = clip_model.get_text_features(input_ids=inputs.input_ids,\n",
    "                                          attention_mask=inputs.attention_mask)\n",
    "    iv = iv / iv.norm(dim=-1, keepdim=True)\n",
    "    tv = tv / tv.norm(dim=-1, keepdim=True)\n",
    "    sims = (iv * tv).sum(dim=-1)\n",
    "    all_sims.append(sims.cpu())\n",
    "clip_score = torch.cat(all_sims).mean().item()\n",
    "print(f\"CLIPScore: {clip_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
